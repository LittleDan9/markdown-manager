{
  "backup_timestamp": "2025-08-13T03:56:17.144215+00:00",
  "database_url": "10.0.1.51:5432/markdown_manager",
  "tables": {
    "users": [{"id":1,"email":"dan@littledan.com","hashed_password":"$argon2id$v=19$m=65536,t=3,p=4$gXCuNUaIUeqdMwZA6H1vLQ$frhXrp3flRhTUSEAX87vM9Oqq5oz3jyiBBjhaLHOhtw","reset_token":null,"reset_token_expires":null,"mfa_enabled":false,"totp_secret":null,"backup_codes":null,"first_name":"Dan","last_name":"Little","display_name":"LittleDan9","bio":"","is_active":true,"is_verified":false,"current_doc_id":3,"sync_preview_scroll_enabled":true,"autosave_enabled":true,"created_at":"2025-07-18T03:39:36.420807-04:00","updated_at":"2025-08-03T05:19:29.699933-04:00"}],
    "documents": [{"id":1,"name":"PBM Hyland OnBase","content":"# Missouri PBM to Hyland OnBase\n\nMissouri PBM will receive paginated reports generated by GainwellGenius's inSight (Power BI) platform. These reports will be created on a scheduled basis using predefined filters and conditions. The outcome of each report will be delivered to the Genius Audit system and ultimately forwarded to Missouri’s Hyland OnBase solution.\n\n---\n\n## Proposed Solution\n\nThe solution depends on the availability of an **SFTP endpoint provided by Missouri PBM**, accessible either via the public Internet or through an existing network connection between Gainwell and the Hyland OnBase environment.\n\n- **Hyland OnBase (On-Premises)** – Requires an SFTP location that is monitored by a **Directory Import Processor (DIP)**.\n- **Hyland Cloud** – Typically provides a native SFTP option through a **Hyland Gateway**.\n\nGainwellGenius inSight will export reports on a scheduled basis to cloud storage. The **data lake** will execute an API call to retrieve these reports and move them into the **Departure system**. The Departure system will create an **audit trail**, recording when files were prepared for transmission, their destination, and the success or failure of each delivery. Furthermore, it will automate retries for files that failed to send and will notify the Genius Operations team of these failures.\n\nFrom there, Departure will send the files to the **Gainwell Managed File Transfer (MFT)** organization. MFT will use automation tools to securely deliver the reports to the Hyland OnBase SFTP endpoint.\n\nThis solution provides a **consistent and reliable delivery mechanism**, ensuring reports are transmitted on time and with an auditable record of all file transfers.\n\n---\n\n#### Flow Diagram: Report Delivery to Hyland OnBase (On-Prem)\n```mermaid\nflowchart TD\n    subgraph PowerBI\n        PBI(Power BI)\n        PA(Power Automate)\n        BLOB(Azure Blob Storage)\n    end\n\n    subgraph Datalake\n        DBX(MO Databricks Workspace)\n        DEP(Departure System)\n        MFT(\"Gainwell MFT (SFTP)\")\n    end\n\n    subgraph Hyland OnBase\n        SFTP(SFTP Folder)\n        DIP(Hyland DIP) \n    end\n\n    PBI --> PA --> |Scheduled Paginated Report Output| BLOB\n    BLOB <--> |Scheduled API Pull| DBX\n    DBX --> |Write to Departure| DEP --> |Audit Information Logged| MFT --> SFTP\n    SFTP <--> DIP\n```","category":"MO","user_id":1,"created_at":"2025-08-02T02:28:54.01081-04:00","updated_at":"2025-08-03T00:28:24.438559-04:00"}, {"id":2,"name":"KHINEnrichment","content":"# KHIN Enrichment\r\n\r\nKDHE has request that Gainwell address their lack of access to the KHIN provided Medicaid Eligibility Enrichment within the their EDW. Gainwell has been asked to be provide a solution within GainwellGenius.\r\n\r\nGainwell we update the process for providing extracts to KHIN for Medicaid Eligibility Enreichment using GainwellGenius using automation. Furthermore, an ETL process must be created by which the enriched data from KHIN is trasnfomred and loaded into the KS EDW Gold Layer to be used for providing meaningful anmalytics dashboards to KDHE.\r\n\r\n---\r\n\r\n## Current Process\r\nThe current process is managed in many different systems with manual intervation at various steps. The main componenets are HealtheIntent, Notepad++, and WinSCP.\r\n\r\n### Extract\r\n\r\nSQL query is manually executed within HealtheIntent by Gainwell Technologies resources on the KS DDI Team every Tuesday morning to extract the Meidcair Eleigibility records, the consolidated CDA Patient Panel. This extract is obtained via export within HealtheIntent and must be named `Consolidated_CDA_Patient_Panel_{MMDDYYYY}.csv`, where `{MMDDYY}` represented the two digit month, two digit data and two digit year (ex. `Consolidated_CDA_Patient_Panel_071825`). The Gainwell resource must then manually modify the extract to remove formatting specificlly added by HealtheIntent (First row header, double quoted lines, etc.). Once the file is prepared for transmission it is sent via SFTP using WinSCP to a desginated folder in KS MOVEit. This specific folder is where KHIN retreives the file for data enrichment. \r\n\r\n### Recevie Enrichments\r\n\r\nEvery Thursday proceeding KHIN's receival of the weekly Tueday export they will sends via SFTP compressed archive file(s) containing XML files with the enriched data in a structure format. The files are delivered to the KS MOVEit servers where they ultimatly remain unused indefinitly. As part of the migration effort from HealtheIntent, the KHIN exports are now routed to the Gainwel MFT and are placed in the KS Landing bucket within GainwellGenius. However, the data that is trigger the creation of these files is still the process defined in the [Extract](#extract) section.\r\n\r\n### Current Dataflow\r\n\r\n```mermaid\r\nflowchart LR\r\n    subgraph HealtheIntent\r\n        e(SQL Export)\r\n        v[(Vertica)]\r\n    end\r\n    subgraph Gainwell\r\n        txt(Editor)\r\n        mft(KS MOVEit)\r\n    end\r\n    subgraph KHIN\r\n        enrich(Enrichment Process)\r\n    end\r\n    e <--> |1: Export - Start| v\r\n    e --> |2: Manual Transformation| txt \r\n    txt --> |3: Manaual File Delivery| mft --> |4: KHIN Data Pull | enrich --> |5: KHIN Data Push - Stop| mft\r\n\r\n    linkStyle 0 stroke:#0f0\r\n    linkStyle 4 stroke:#f00\r\n```\r\n\r\n## Migration to Genius\r\n\r\nDur in part to the KDHE request as well as the migration to GainwellGenius as the EDW there is an opportunity to automate the KHIN enrichment process using a Notbook, Job, and Scheduler. In order to be describe the process it is best to split the enrichement into two key componenets **Shipping** and **Receiving**.\r\n\r\n### Shipping - Data Extract to KHIN\r\n\r\nThe process of extracting can be simplified. The SQL query that is ued to produce the output CSV can be altered slightly to accecpt input paramters to defeine the data range that will be used to limit the data extract. Here is a detailed plan for migrating the manual SQL into a Databricks friendly approach for automation:\r\n\r\n#### Create a Notebook \r\n\r\nThe KHIN Extract Notebook should be parameterized with widgets specifically for the data limited SQL conditions. The Notbook will be a mix of Python and SQL. The notbook should be developed using source control and pushed into Databricks us GitHub CI/CD.\r\n\r\n##### Cell 1\r\n\r\nThe first cell will setup the job for run by creating manual input widgets or dynamically computing the data conditionals. Here is an example of the fist cell:\r\n\r\n```python\r\nimport pre \r\n\r\nfrom datetime import date\r\nfrom dateutil.relativedelta import relativedelta\r\n\r\n# 1. Compute “first” & “last” of previous month\r\ntoday = date.today()\r\nfirst_of_this_month = today.replace(day=1)\r\nend_of_prev_month    = first_of_this_month - relativedelta(days=1)\r\nstart_of_prev_month  = end_of_prev_month.replace(day=1)\r\n\r\nauto_start = start_of_prev_month.isoformat()  # e.g. \"2025-06-01\"\r\nauto_end   = end_of_prev_month.isoformat()    # e.g. \"2025-06-30\"\r\n\r\n# 2. Get the workspace URL to determine Environment and Region\r\n\r\n# Create Widgets with empty values\r\ndbutils.widgets.text(\"env\",    \"\",    \"Environment\")\r\ndbutils.widgets.text(\"region\", \"\", \"Region\")\r\n\r\nenv_input    = dbutils.widgets.get(\"env\")    or env\r\nregion_input = dbutils.widgets.get(\"region\") or region\r\n\r\nif env_input and region_input:\r\n    env, region = env_input, region_input\r\nelse:\r\n    workspace_url = dbutils.notebook.getContext().apiUrl().get() \r\n    host = workspace_url.split(\"://\")[-1].split(\"/\", 1)[0]\r\n    env, region = m.group(1), m.group(2)\r\n    m = re.match(r'^[^.]+-([^-]+)-([^-]+)\\.', host)\r\n    if not m:\r\n        raise RuntimeError(f\"Unexpected workspace URL format: {workspace_url}\")\r\n\r\n\r\n# 3. Create widgets with those computed values as defaults\r\ndbutils.widgets.text(\"start_date\", auto_start, \"Start Date\")\r\ndbutils.widgets.text(\"end_date\",   auto_end,   \"End Date\")\r\n\r\n# 4. Read data from widget setting automatic values if someone cleared the default input.\r\n\r\nsd = dbutils.widgets.get(\"start_date\") or auto_start\r\ned = dbutils.widgets.get(\"end_date\")   or auto_end\r\n\r\nprint(f\"Using date range: {sd} → {ed}\")\r\nprint(f\"Running in {env} environment in region {region}\")\r\n\r\n```\r\n\r\nThis will setup default values for the SQL `WHERE` clause that are the previous month's first and last day as `sd` and `ed`. Additionally it exposes `env` (environment) and `region` (aws region) to dynamically composed the output bucket for the exported csv.\r\n\r\n##### Cell 2\r\n\r\nThis is where the existing SQL statement will live, it will be a executed with Python using pySpark and Pandas. The following is a code bloc for what you might see in this cell with a sample query\r\n\r\n```python\r\nfrom datetime import date\r\n\r\ndf = spark.sql(f\"\"\"\r\n  SELECT *\r\n  FROM your_table\r\n  WHERE date_col BETWEEN '{sd}' AND '{ed}'\r\n\"\"\")\r\n\r\npdf = df.toPandas()\r\n```\r\n\r\nNote the variables `${sd}` and `${ed}` in the SQL Statement and how they map to the variable names defined in the python code snip above for cell 1.\r\n\r\n\r\n##### Cell 3\r\n\r\nIn this cell the export to CSV and write to the Genius Departure bucket will occur. Herei s an example of hwo the script would work.\r\n\r\n```python\r\nimport io, boto3\r\n\r\nbuf = io.StringIO()\r\npdf.to_csv(buf, index=False)\r\n\r\nformatted_date = date.today().strftime(\"%m%d%y\")\r\n\r\ns3 = boto3.client(\"s3\")\r\ns3.put_object(\r\n    Bucket=\"gia-{env}-ks-{region}-data-departure\",\r\n    Key=f\"outbound/khin/Consolidated_CDA_Patient_Panel_{formatted_date}.csv\",\r\n    Body=buf.getvalue()\r\n)\r\n\r\nprint(f\"Wrote {len(pdf)} rows to s3://gia-{env}-ks-{region}-data-departure/outbound/khin/Consolidated_CDA_Patient_Panel_{formatted_date}.csv\")\r\n\r\n```\r\n\r\n#### Schedule the Job\r\n\r\nWith the worksbook saved a job can be created. You jobs are defeind in a YAML file and that file is stored in your GitHub repository. It is then uploaded to Databricks. The job is configured to run as a Service Principal that has access to query the required tables as well trigger your workbooks to run on your Warehoure.\r\n\r\nHere is a sample YAML file that runs a Notebook at 8:00 AM evey Tuesday.\r\n\r\n```YAML\r\n# jobs/export_prev_month.yaml\r\nname: \"ExportPrevMonthJob\"\r\n\r\n# Use either a new_cluster spec or reference an existing_cluster_id\r\nexisting_cluster_id: \"<The ID of the Cluster>\"\r\n\r\ntasks:\r\n  - task_key: \"export_prev_month\"\r\n    notebook_task:\r\n      notebook_path: \"/Users/you/ExportPrevMonth\"\r\n\r\n# Schedule block using Quartz cron syntax:\r\nschedule:\r\n  quartz_cron_expression: \"0 0 8 ? * TUE *\"\r\n  timezone_id: \"America/New_York\"\r\n  pause_status: \"UNPAUSED\"\r\n```\r\n\r\nThe key elements here are your `existing_cluster_id` and `notebook_path`. This will need to be set accordingly and can be obtained by environment from a Genius Databricks Admin.\r\n\r\n## Receiving - Getting the Enriched Data\r\n\r\nWhen data is received from KHIN it is already being transfered to Gainwell MFT and delivered to the KS Genius Landing Bucket. If not done so already, you will work with the Databricks team, through a Service Now request to processes these files into Raw. The ingestion pattern to Raw and extract the XML files from their origianl format and being the process of tracking this data in our Audit, Balance, and Control database. Additionally, unaltered original source files are retained in Landing for non-repudiation needs.\r\n\r\n### ETL - Extract, Transform, Load\r\n\r\nFollwing a simialr process to the export, the KS team will need to produce ETL notebooks that will read in the KHIN XML data and structure from the KS Landing bucket, transform it into tablular format for writting to the bronze layer, laoding it to bronze. This might involve creating a Sprak Dataframe, executing any transfomrations, and writting the result into the bronze layer as a Delta Table.\r\n\r\nThe following an example of how you would use pySprak to read the XML and perofrm any tranformations and producing a dataframe that could then be written to you bronze layer.\r\n\r\n```xml\r\n<!-- Example XML -->\r\n<records>\r\n  <record id=\"123\" status=\"active\">\r\n    <name>Acme Corp</name>\r\n    <revenue>42.5</revenue>\r\n  </record>\r\n  <record id=\"456\" status=\"inactive\">\r\n    <name>Jim's Diner</name>\r\n    <revenue>12.23</revenue>\r\n  </record>\r\n</records>\r\n\r\n```\r\n\r\n```python\r\n# Create a list to store each dataframe per file\r\ndfs = []\r\nfor xml_file in files:\r\n    df = spark.read.format(\"xml\") \\\r\n        .option(\"rowTag\", \"record\") \\\r\n        .option(\"attrbiutePrefix\", \"\") \\\r\n        .option(\"valueTag\", \"_VALUE\") \\\r\n        .load(xml_file) \\\r\n        .withColumn(\"source_file\", input_file_name())  # This would be a column in your table\r\n        .withColumn(\"event_time\", to_timestamp(\"timeString\", \"yyyy-MM-dd'T'HH:mm:ss\"))\r\n    dfs.append(df)\r\n```\r\n\r\nA key call out here is the `.withColum(..)` let's you inject additional columns into your dataframe to retain infomration about the record that may not be included in the XML file itself. In the example we've added the original files name that produced the record as well as a date/time stamp for when the row was ingested.\r\n\r\nBased on the sample XML above and the creation of the dataframe, you'd expect to the a table with the following output.\r\n\r\n\r\n|source_file               |id |status|name     |revenue|\r\n|---                       |---|---   |---      |---    |\r\n|s3://…/xml/file1.xml      |123|active|Acme Corp|42.5   |\r\n|s3://…/xml/file1.xml      |456|inactive|Jim's Diner|12.23|\r\n\r\n","category":"KS","user_id":1,"created_at":"2025-07-18T13:22:12-04:00","updated_at":"2025-08-01T10:07:16.834935-04:00"}, {"id":3,"name":"KHIN Enrichment","content":"# KHIN Enrichment\n\nKDHE has request that Gainwell address their lack of access to the KHIN provided Medicaid Eligibility Enrichements within the their EDW. Gainwell has been asked to be provide a solution within GainwellGenius.\n\nGainwell we update the process for providing extracts to KHIN for Medicaid Eleigibility Enrichment using GainwellGenius using automation. Furthermore, an ETL process must be created by which the enriched data from KHIN is trasnfomred and loaded into the KS EDW Gold Layer to be used for providing meaningful anmalytics dashboards to KDHE.\n\n---\n\n## Current Process\nThe current process is managed in many different systems with manual intervation at various steps. The main components are HealtheIntent, Notepad++, and WinSCP.\n\n### Extract\n\nSQL query is manually executed within HealtheIntent by Gainwell Technologies resources on the KS DDI Team every Tuesday morning to extract the Meidcair Eligibility records, the consolidated CDA Patient Panel. This extract is obtained via export within HealtheIntent and must be named `Consolidated_CDA_Patient_Panel_{MMDDYYYY}.csv`, where `{MMDDYY}` represented the two digit month, two digit data and two digit year (ex. `Consolidated_CDA_Patient_Panel_071825`). The Gainwell resource must then manually modify the extract to remove formatting specifically added by HealtheIntent (First row header, double quoted lines, etc.). Once the file is prepared for transmission it is sent via SFTP using WinSCP to a designated folder in KS MOVEit. This specific folder is where KHIN retrieves the file for data enrichment. \n\n### Recevie Enrichments\n\nEvery Thursday proceeding KHIN's receival of the weekly Tuesday export they will sends via SFTP compressed archive file(s) containing XML files with the enriched data in a structure format. The files are delivered to the KS MOVEit servers where they ultimately remain unused indefinitely. As part of the migration effort from HealtheIntent, the KHIN exports are now routed to the Gainwell  MFT and are placed in the KS Landing bucket within GainwellGenius. However, the data that is trigger the creation of these files is still the process defined in the [Extract](#extract) section.\n\n### Current Dataflow\n\n```mermaid\nflowchart LR\n    subgraph HealtheIntent\n        e(SQL Export)\n        v[(Vertica)]\n    end\n    subgraph Gainwell\n        txt(Editor)\n        mft(KS MOVEit)\n    end\n    subgraph KHIN\n        enrich(Enrichment Process)\n    end\n    e <--> |1: Export - Start| v\n    e --> |2: Manual Transformation| txt \n    txt --> |3: Manaual File Delivery| mft --> |4: KHIN Data Pull | enrich --> |5: KHIN Data Push - Stop| mft\n\n    linkStyle 0 stroke:#0f0\n    linkStyle 4 stroke:#f00\n```\n\n## Migration to Genius\n\nDue in part to the KDHE request as well as the migration to GainwellGenius as the EDW there is an opportunity to automate the KHIN enrichment process using a Notebook, Job, and Scheduler. In order to be describe the process it is best to split the enrichment into two key components **Shipping** and **Receiving**.\n\n### Shipping - Data Extract to KHIN\n\nThe process of extracting can be simplified. The SQL query that is ued to produce the output CSV can be altered slightly to accept input parameters to defeine the data range that will be used to limit the data extract. Here is a detailed plan for migrating the manual SQL into a Databricks friendly approach for automation:\n\n#### Create a Notebook \n\nThe KHIN Extract Notebook should be parameterized with widgets specifically for the data limited SQL conditions. The Notbook will be a mix of Python and SQL. The notbook should be developed using source control and pushed into Databricks us GitHub CI/CD.\n\n##### Cell 1\n\nThe first cell will setup the job for run by creating manual input widgets or dynamically computing the data conditionals. Here is an example of the fist cell:\n\n```python\nimport pre \n\nfrom datetime import date\nfrom dateutil.relativedelta import relativedelta\n\n# 1. Compute “first” & “last” of previous month\ntoday = date.today()\nfirst_of_this_month = today.replace(day=1)\nend_of_prev_month    = first_of_this_month - relativedelta(days=1)\nstart_of_prev_month  = end_of_prev_month.replace(day=1)\n\nauto_start = start_of_prev_month.isoformat()  # e.g. \"2025-06-01\"\nauto_end   = end_of_prev_month.isoformat()    # e.g. \"2025-06-30\"\n\n# 2. Get the workspace URL to determine Environment and Region\n\n# Create Widgets with empty values\ndbutils.widgets.text(\"env\",    \"\",    \"Environment\")\ndbutils.widgets.text(\"region\", \"\", \"Region\")\n\nenv_input    = dbutils.widgets.get(\"env\")    or env\nregion_input = dbutils.widgets.get(\"region\") or region\n\nif env_input and region_input:\n    env, region = env_input, region_input\nelse:\n    workspace_url = dbutils.notebook.getContext().apiUrl().get() \n    host = workspace_url.split(\"://\")[-1].split(\"/\", 1)[0]\n    env, region = m.group(1), m.group(2)\n    m = re.match(r'^[^.]+-([^-]+)-([^-]+)\\.', host)\n    if not m:\n        raise RuntimeError(f\"Unexpected workspace URL format: {workspace_url}\")\n\n\n# 3. Create widgets with those computed values as defaults\ndbutils.widgets.text(\"start_date\", auto_start, \"Start Date\")\ndbutils.widgets.text(\"end_date\",   auto_end,   \"End Date\")\n\n# 4. Read data from widget setting automatic values if someone cleared the default input.\n\nsd = dbutils.widgets.get(\"start_date\") or auto_start\ned = dbutils.widgets.get(\"end_date\")   or auto_end\n\nprint(f\"Using date range: {sd} → {ed}\")\nprint(f\"Running in {env} environment in region {region}\")\n\n```\n\nThis will setup default values for the SQL `WHERE` clause that are the previous month's first and last day as `sd` and `ed`. Additionally it exposes `env` (environment) and `region` (aws region) to dynamically composed the output bucket for the exported csv.\n\n##### Cell 2\n\nThis is where the existing SQL statement will live, it will be a executed with Python using pySpark and Pandas. The following is a code bloc for what you might see in this cell with a sample query\n\n```python\nfrom datetime import date\n\ndf = spark.sql(f\"\"\"\n  SELECT *\n  FROM your_table\n  WHERE date_col BETWEEN '{sd}' AND '{ed}'\n\"\"\")\n\npdf = df.toPandas()\n```\n\nNote the variables `${sd}` and `${ed}` in the SQL Statement and how they map to the variable names defined in the python code snip above for cell 1.\n\n\n##### Cell 3\n\nIn this cell the export to CSV and write to the Genius Departure bucket will occur. Herei s an example of hwo the script would work.\n\n```python\nimport io, boto3\n\nbuf = io.StringIO()\npdf.to_csv(buf, index=False)\n\nformatted_date = date.today().strftime(\"%m%d%y\")\n\ns3 = boto3.client(\"s3\")\ns3.put_object(\n    Bucket=\"gia-{env}-ks-{region}-data-departure\",\n    Key=f\"outbound/khin/Consolidated_CDA_Patient_Panel_{formatted_date}.csv\",\n    Body=buf.getvalue()\n)\n\nprint(f\"Wrote {len(pdf)} rows to s3://gia-{env}-ks-{region}-data-departure/outbound/khin/Consolidated_CDA_Patient_Panel_{formatted_date}.csv\")\n\n```\n\n#### Schedule the Job\n\nWith the worksbook saved a job can be created. You jobs are defeind in a YAML file and that file is stored in your GitHub repository. It is then uploaded to Databricks. The job is configured to run as a Service Principal that has access to query the required tables as well trigger your workbooks to run on your Warehoure.\n\nHere is a sample YAML file that runs a Notebook at 8:00 AM evey Tuesday.\n\n```YAML\n# jobs/export_prev_month.yaml\nname: \"ExportPrevMonthJob\"\n\n# Use either a new_cluster spec or reference an existing_cluster_id\nexisting_cluster_id: \"<The ID of the Cluster>\"\n\ntasks:\n  - task_key: \"export_prev_month\"\n    notebook_task:\n      notebook_path: \"/Users/you/ExportPrevMonth\"\n\n# Schedule block using Quartz cron syntax:\nschedule:\n  quartz_cron_expression: \"0 0 8 ? * TUE *\"\n  timezone_id: \"America/New_York\"\n  pause_status: \"UNPAUSED\"\n```\n\nThe key elements here are your `existing_cluster_id` and `notebook_path`. This will need to be set accordingly and can be obtained by environment from a Genius Databricks Admin.\n\n## Receiving - Getting the Enriched Data\n\nWhen data is received from KHIN it is already being transfered to Gainwell MFT and delivered to the KS Genius Landing Bucket. If not done so already, you will work with the Databricks team, through a Service Now request to processes these files into Raw. The ingestion pattern to Raw and extract the XML files from their origianl format and being the process of tracking this data in our Audit, Balance, and Control database. Additionally, unaltered original source files are retained in Landing for non-repudiation needs.\n\n### ETL - Extract, Transform, Load\n\nFollwing a simialr process to the export, the KS team will need to produce ETL notebooks that will read in the KHIN XML data and structure from the KS Landing bucket, transform it into tablular format for writing to the bronze layer, laoding it to bronze. This might involve creating a Sprak Dataframe, executing any transfomrations, and writting the result into the bronze layer as a Delta Table.\n\nThe following an example of how you would use pySprak to read the XML and perofrm any tranformations and producing a dataframe that could then be written to you bronze layer.\n\n```xml\n<!-- Example XML -->\n<records>\n  <record id=\"123\" status=\"active\">\n    <name>Acme Corp</name>\n    <revenue>42.5</revenue>\n  </record>\n  <record id=\"456\" status=\"inactive\">\n    <name>Jim's Diner</name>\n    <revenue>12.23</revenue>\n  </record>\n</records>\n\n```\n\n```python\n# Create a list to store each dataframe per file\ndfs = []\nfor xml_file in files:\n    df = spark.read.format(\"xml\") \\\n        .option(\"rowTag\", \"record\") \\\n        .option(\"attrbiutePrefix\", \"\") \\\n        .option(\"valueTag\", \"_VALUE\") \\\n        .load(xml_file) \\\n        .withColumn(\"source_file\", input_file_name())  # This would be a column in your table\n        .withColumn(\"event_time\", to_timestamp(\"timeString\", \"yyyy-MM-dd'T'HH:mm:ss\"))\n    dfs.append(df)\n```\n\nA key call out here is the `.withColum(..)` let's you inject additional columns into your dataframe to retain infomration about the record that may not be included in the XML file itself. In the example we've added the original files name that produced the record as well as a date/time stamp for when the row was ingested.\n\nBased on the sample XML above and the creation of the dataframe, you'd expect to the a table with the following output.\n\n\n|source_file               |id |status|name     |revenue|\n|---                       |---|---   |---      |---    |\n|s3://…/xml/file1.xml      |123|active|Acme Corp|42.5   |\n|s3://…/xml/file1.xml      |456|inactive|Jim's Diner|12.23|\n\n","category":"KS","user_id":1,"created_at":"2025-07-18T16:52:49-04:00","updated_at":"2025-08-02T02:36:36.586011-04:00"}, {"id":4,"name":"__category_placeholder__","content":"","category":"Genius","user_id":1,"created_at":"2025-07-23T14:40:25-04:00","updated_at":"2025-07-23T14:40:25-04:00"}, {"id":5,"name":"Business Glossary","content":"# Genius Business Dictionary\r\n\r\nThe Genius Business Dictionary will be used to provide internal Gainwell Genius and Genius Affiliates a data Dictionary/Glossary of Schema/Tables and mappings for each. \r\n\r\n---\r\n\r\n## Key Resources \r\n\r\n|Name|Email|Role|\r\n|---|---|---|\r\n|Shannon Winterton|shannon.winterton@gainwelltechnologies.com|Project Lead|\r\n|Devesh Nagar|devesh.nagar@gainwelltechnologies.com|Product Engineer|\r\n\r\n\r\n## System Design\r\n\r\nThe current system Design\r\n\r\n### Frontend - UI\r\n\r\nBusiness Glossary is build on React.js with it's code base in a GitHub Repository.\r\n\r\n### Backend - Database\r\n\r\nThere is a CSV data component which drive the content visible within the React app. This CSV is currently produced manually using a python script that reads tables in Databricks and outputs the CSV which is then deployed along side the frontend.\r\n\r\n### Authentication and Authorization\r\n\r\nThe site today implements a rudimentary shared password concept to access the entire site. There is a single password and that is most likely hard coded in the app or within the CSV. Details were not gathered as this was a red flag that would need addressed immediately.\r\n\r\n### Deployment / Hosting\r\nThe site lives in a S3 bucket acting as a web host within a Gainwell Owned AWS account. T\r\n\r\n---\r\n\r\n## Recommendations\r\n\r\nAfter discussion I am recommend at 2 Phase approach due o t\r\n\r\n### Phase 1\r\n\r\nSince the site is published today with out a traditional backend and is living in S3, the idea with a phase 1 approach would be to use the existing S3 Hosted Terraform Modules/Stacks and create an endpoint for this product. \r\n\r\n#### Environments\r\nThe new endpoint was decided to be http://glossary.gainwellgenius.com.\r\n\r\n### Phase 2\r\n1. Okta Integration for \r\n1. Hosted in ECS","category":"Genius","user_id":1,"created_at":"2025-07-23T14:40:38-04:00","updated_at":"2025-07-28T12:11:31.607694-04:00"}, {"id":6,"name":"GW360 DNS","content":"# Gainwell 360 DNS\r\n\r\nThis should shows how things should be connected\r\n\r\n```mermaid\r\nflowchart TB\r\n    EHC(\"East HealthCheck\") --> c\r\n    root --> d(\"anyltics-uw2.uat.gwtaciuty.com/gw-ui\") --> UIW(\"Nginx on UI COntainer UW2\")   \r\n    root(\"analytics.uat.gwtacuity.com\") --> c(\"anyltics-ue1.uat.gwtaciuty.com/gw-ui\") --> UIE(\"Nginx on UI COntainer UE1\")    \r\n    WHC(\"West HealthCheck\") --> d\r\n    a(\"portal.uat.gainwellgenius.com\") --> portal-ue1.uat.gainwellgenius.com & portal-uw2.uat.gainwellgenius.com\r\n    \r\n    UIW & UIE --> ERoot\r\n\r\n    ERoot(\"gw-ui.certilytics.uat.gainwellgenius.com\") --> a \r\n```","category":"Genius","user_id":1,"created_at":"2025-07-24T18:30:04-04:00","updated_at":"2025-07-24T19:00:04.243486-04:00"}, {"id":7,"name":"Untitled Document","content":"","category":"Drafts","user_id":1,"created_at":"2025-07-24T20:56:24-04:00","updated_at":"2025-07-24T20:56:24-04:00"}],
    "custom_dictionaries": [{"id":1,"user_id":1,"word":"databricks","notes":"Vendor Name","created_at":"2025-07-25T13:46:51.804014-04:00","updated_at":"2025-07-25T13:48:54.119377-04:00"}, {"id":2,"user_id":1,"word":"okta","notes":null,"created_at":"2025-07-25T13:47:04.355285-04:00","updated_at":"2025-07-25T13:47:04.355289-04:00"}, {"id":3,"user_id":1,"word":"gainwell","notes":null,"created_at":"2025-07-27T09:24:20.002328-04:00","updated_at":"2025-07-27T09:24:20.002331-04:00"}, {"id":4,"user_id":1,"word":"gainwellgenius","notes":null,"created_at":"2025-07-27T09:26:05.251273-04:00","updated_at":"2025-07-27T09:26:05.251276-04:00"}],
    "document_recovery": []
  }
}

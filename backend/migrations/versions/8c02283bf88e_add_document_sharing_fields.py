"""Add document sharing fields

Revision ID: 8c02283bf88e
Revises: ec3a0e148d1f
Create Date: 2025-08-15 18:19:33.038356

"""

from typing import Sequence, Union

import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

from alembic import op

# revision identifiers, used by Alembic.
revision: str = "8c02283bf88e"
down_revision: Union[str, Sequence[str], None] = "ec3a0e148d1f"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.alter_column(
        "categories",
        "created_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        type_=sa.DateTime(),
        existing_nullable=False,
        existing_server_default=sa.text("CURRENT_TIMESTAMP"),
    )
    op.alter_column(
        "categories",
        "updated_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        type_=sa.DateTime(),
        existing_nullable=False,
        existing_server_default=sa.text("CURRENT_TIMESTAMP"),
    )
    op.alter_column(
        "custom_dictionaries",
        "created_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        type_=sa.DateTime(),
        nullable=False,
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "custom_dictionaries",
        "updated_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        type_=sa.DateTime(),
        nullable=False,
        existing_server_default=sa.text("now()"),
    )

    # Safely drop index if it exists
    connection = op.get_bind()
    inspector = sa.inspect(connection)
    indexes = inspector.get_indexes("custom_dictionaries")
    index_names = [idx["name"] for idx in indexes]

    if "uq_user_dictionary_word" in index_names:
        op.drop_index(
            op.f("uq_user_dictionary_word"),
            table_name="custom_dictionaries",
            postgresql_where="(category_id IS NULL)",
        )

    op.create_index(
        op.f("ix_custom_dictionaries_user_id"),
        "custom_dictionaries",
        ["user_id"],
        unique=False,
    )
    op.drop_constraint(
        op.f("custom_dictionaries_user_id_fkey"),
        "custom_dictionaries",
        type_="foreignkey",
    )
    op.create_foreign_key(
        None, "custom_dictionaries", "users", ["user_id"], ["id"], ondelete="CASCADE"
    )
    op.add_column(
        "documents", sa.Column("share_token", sa.String(length=64), nullable=True)
    )

    # Add is_shared column with default value first, then make it NOT NULL
    op.add_column(
        "documents",
        sa.Column(
            "is_shared", sa.Boolean(), nullable=True, server_default=sa.text("FALSE")
        ),
    )

    # Update any existing NULL values to FALSE
    connection = op.get_bind()
    connection.execute(
        sa.text("UPDATE documents SET is_shared = FALSE WHERE is_shared IS NULL")
    )

    # Now make the column NOT NULL
    op.alter_column("documents", "is_shared", nullable=False)
    op.alter_column(
        "documents",
        "created_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=False,
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "documents",
        "updated_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=False,
        existing_server_default=sa.text("now()"),
    )
    op.create_index(
        op.f("ix_documents_category"), "documents", ["category"], unique=False
    )
    op.create_index(op.f("ix_documents_name"), "documents", ["name"], unique=False)
    op.create_index(
        op.f("ix_documents_share_token"), "documents", ["share_token"], unique=True
    )
    op.create_index(
        op.f("ix_documents_user_id"), "documents", ["user_id"], unique=False
    )
    op.create_unique_constraint("uq_user_name", "documents", ["user_id", "name"])
    op.alter_column(
        "users",
        "reset_token_expires",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        type_=sa.DateTime(),
        existing_nullable=True,
    )
    op.alter_column("users", "mfa_enabled", existing_type=sa.BOOLEAN(), nullable=False)
    op.alter_column(
        "users",
        "backup_codes",
        existing_type=postgresql.JSON(astext_type=sa.Text()),
        type_=sa.Text(),
        existing_nullable=True,
    )
    op.alter_column(
        "users",
        "sync_preview_scroll_enabled",
        existing_type=sa.BOOLEAN(),
        nullable=False,
    )
    op.alter_column(
        "users", "autosave_enabled", existing_type=sa.BOOLEAN(), nullable=False
    )
    op.alter_column("users", "is_active", existing_type=sa.BOOLEAN(), nullable=False)
    op.alter_column("users", "is_verified", existing_type=sa.BOOLEAN(), nullable=False)
    op.alter_column(
        "users",
        "created_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        type_=sa.DateTime(),
        nullable=False,
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "users",
        "updated_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        type_=sa.DateTime(),
        nullable=False,
        existing_server_default=sa.text("now()"),
    )
    op.create_foreign_key(
        None, "users", "documents", ["current_doc_id"], ["id"], ondelete="SET NULL"
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_constraint(None, "users", type_="foreignkey")
    op.alter_column(
        "users",
        "updated_at",
        existing_type=sa.DateTime(),
        type_=postgresql.TIMESTAMP(timezone=True),
        nullable=True,
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "users",
        "created_at",
        existing_type=sa.DateTime(),
        type_=postgresql.TIMESTAMP(timezone=True),
        nullable=True,
        existing_server_default=sa.text("now()"),
    )
    op.alter_column("users", "is_verified", existing_type=sa.BOOLEAN(), nullable=True)
    op.alter_column("users", "is_active", existing_type=sa.BOOLEAN(), nullable=True)
    op.alter_column(
        "users", "autosave_enabled", existing_type=sa.BOOLEAN(), nullable=True
    )
    op.alter_column(
        "users",
        "sync_preview_scroll_enabled",
        existing_type=sa.BOOLEAN(),
        nullable=True,
    )
    op.alter_column(
        "users",
        "backup_codes",
        existing_type=sa.Text(),
        type_=postgresql.JSON(astext_type=sa.Text()),
        existing_nullable=True,
    )
    op.alter_column("users", "mfa_enabled", existing_type=sa.BOOLEAN(), nullable=True)
    op.alter_column(
        "users",
        "reset_token_expires",
        existing_type=sa.DateTime(),
        type_=postgresql.TIMESTAMP(timezone=True),
        existing_nullable=True,
    )
    op.drop_constraint("uq_user_name", "documents", type_="unique")
    op.drop_index(op.f("ix_documents_user_id"), table_name="documents")
    op.drop_index(op.f("ix_documents_share_token"), table_name="documents")
    op.drop_index(op.f("ix_documents_name"), table_name="documents")
    op.drop_index(op.f("ix_documents_category"), table_name="documents")
    op.alter_column(
        "documents",
        "updated_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=True,
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "documents",
        "created_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=True,
        existing_server_default=sa.text("now()"),
    )
    op.drop_column("documents", "is_shared")
    op.drop_column("documents", "share_token")
    op.drop_constraint(None, "custom_dictionaries", type_="foreignkey")
    op.create_foreign_key(
        op.f("custom_dictionaries_user_id_fkey"),
        "custom_dictionaries",
        "users",
        ["user_id"],
        ["id"],
    )
    op.drop_index(
        op.f("ix_custom_dictionaries_user_id"), table_name="custom_dictionaries"
    )

    # Safely create index if it doesn't exist
    connection = op.get_bind()
    inspector = sa.inspect(connection)
    indexes = inspector.get_indexes("custom_dictionaries")
    index_names = [idx["name"] for idx in indexes]

    if "uq_user_dictionary_word" not in index_names:
        op.create_index(
            op.f("uq_user_dictionary_word"),
            "custom_dictionaries",
            ["user_id", "word"],
            unique=True,
            postgresql_where="(category_id IS NULL)",
        )
    op.alter_column(
        "custom_dictionaries",
        "updated_at",
        existing_type=sa.DateTime(),
        type_=postgresql.TIMESTAMP(timezone=True),
        nullable=True,
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "custom_dictionaries",
        "created_at",
        existing_type=sa.DateTime(),
        type_=postgresql.TIMESTAMP(timezone=True),
        nullable=True,
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "categories",
        "updated_at",
        existing_type=sa.DateTime(),
        type_=postgresql.TIMESTAMP(timezone=True),
        existing_nullable=False,
        existing_server_default=sa.text("CURRENT_TIMESTAMP"),
    )
    op.alter_column(
        "categories",
        "created_at",
        existing_type=sa.DateTime(),
        type_=postgresql.TIMESTAMP(timezone=True),
        existing_nullable=False,
        existing_server_default=sa.text("CURRENT_TIMESTAMP"),
    )
    # ### end Alembic commands ###
